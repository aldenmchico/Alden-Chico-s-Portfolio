{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore FutureWarning warnings that may pop up from importing following libraries\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Facial_Recognition_Module.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Libraries from last part of the project\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# Importing functions from the last part of the project's ipynb file as a library for this part\n",
    "import import_ipynb\n",
    "from Facial_Recognition_Module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Display all images from a given list '''\n",
    "\n",
    "def show_all_images(img_list, columns=4):\n",
    "    \n",
    "    # Shape matplotlib subplot\n",
    "    size = len(img_list)\n",
    "    rows = int(size / columns)\n",
    "    if size % columns  != 0 or rows == 0:\n",
    "        rows += 1\n",
    "    \n",
    "    figsize = (10, 5)\n",
    "    fig, axs = plt.subplots(rows, columns, figsize=figsize)\n",
    "    \n",
    "    # Convert grayscale images to BGR\n",
    "    if img_list[0].shape[-1] != 3:\n",
    "        for idx, img in enumerate(img_list):\n",
    "            img_list[idx] = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Draw all the images if more than one row can be drawn with given column number\n",
    "    i = 0\n",
    "    if rows > 1:\n",
    "        for row in range(rows):\n",
    "            for column in range(columns):\n",
    "                if i >= len(img_list):\n",
    "                    axs[row][column].axis('off')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                img = img_list[i]\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                _ = axs[row][column].imshow(img)\n",
    "                _ = axs[row][column].axis('off')\n",
    "                i += 1\n",
    "    \n",
    "    # Draw all images if images only take up one row\n",
    "    else:\n",
    "        for column in range(columns):\n",
    "            if i >= len(img_list):\n",
    "                    _ = axs[column].axis('off')\n",
    "                    i += 1\n",
    "                    continue\n",
    "            img = img_list[i]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            _ = axs[column].imshow(img)\n",
    "            _ = axs[column].axis('off')\n",
    "            i += 1 \n",
    "        \n",
    "        \n",
    "    _ = plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling Images to Image Size Median for Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Resize the face images to the median face image size from the list'''\n",
    "\n",
    "def normalize_face_image_size(gray_face_list):\n",
    "    \n",
    "    # Gather all the width/height dimensions from the face images\n",
    "    x_size_list = y_size_list = []\n",
    "    for face in gray_face_list:\n",
    "        x_size_list.append(face.shape[0])\n",
    "        y_size_list.append(face.shape[1])\n",
    "    \n",
    "    # Find the median from the width/height dimensions\n",
    "    x_median = int(np.median(x_size_list))\n",
    "    y_median = int(np.median(y_size_list))\n",
    "    dim = (x_median, y_median)\n",
    "    \n",
    "    # Resize the face image to the median face image size\n",
    "    rescale_face_list = []\n",
    "    for face in gray_face_list:\n",
    "        if face.shape[0] == 0 or face.shape[1] == 0:\n",
    "            gray_face_list.remove(face)\n",
    "            continue\n",
    "        rescale_face = cv2.resize(face, dim)\n",
    "        rescale_face_list.append(rescale_face)\n",
    "        \n",
    "    return rescale_face_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Draw side by side image comparisons using matplotlib '''\n",
    "\n",
    "def compare_two_images(img_1, img_2):\n",
    "    \n",
    "    # Create subplots plot for a 1x2 image subplot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Display the first image\n",
    "    _ = axs[0].imshow(img_1, cmap='gray')\n",
    "    _ = axs[0].axis('off')\n",
    "    \n",
    "    # Display the second image\n",
    "    _ = axs[1].imshow(img_2, cmap='gray')\n",
    "    _ = plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Apply preprocessing steps to provided face image list '''\n",
    "\n",
    "def preprocess_images(gray_face_list, preprocess_steps):\n",
    "    \n",
    "    # Make a copy of the face image list\n",
    "    process_face_list = gray_face_list.copy()\n",
    "    \n",
    "    # If there are no preprocessing steps provided, return the original list\n",
    "    if len(preprocess_steps) == 0:\n",
    "        return process_face_list\n",
    "    \n",
    "    # Apply the preprocessing steps provided in the list to the list\n",
    "    for step in preprocess_steps: \n",
    "        process_face_list = step(process_face_list)\n",
    "    return process_face_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Detect faces from original images and apply preprocessing steps\n",
    "    Organize the images into defaultdicts where value entries are lists containing original and face images\n",
    "'''\n",
    "\n",
    "def prepare_image_dictionaries(original_list, label_list, preprocess_steps):\n",
    "    \n",
    "    img_dict = defaultdict(list)\n",
    "    face_dict = defaultdict(list)\n",
    "    gray_face_list = []\n",
    "    \n",
    "    # Organize the original image and face lists in the img_dict/face_dict by their corresponding labels\n",
    "    for img, label in zip(original_list, label_list):\n",
    "        faces, _ = detect_faces(img)\n",
    "        if faces is not None:\n",
    "            img_dict[label].append(img)\n",
    "            face_dict[label].append(faces[0])\n",
    "    \n",
    "    # Add all the faces from the face dictionary to a gray_face_list super list\n",
    "    for _, face_list in face_dict.items():\n",
    "        for face in face_list:\n",
    "            gray_face_list.append(face)\n",
    "    \n",
    "    # Process all the faces in the super list\n",
    "    process_face_list = preprocess_images(gray_face_list, preprocess_steps)\n",
    "    \n",
    "    # Store the processed faces to the face dictionary\n",
    "    start = end = 0\n",
    "    for label, img_list in img_dict.items():\n",
    "        start, end = end, end + len(img_list)\n",
    "        face_dict[label] = process_face_list[start:end]\n",
    "  \n",
    "    return img_dict, face_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Apply a Gaussian blur filter to the face list'''\n",
    "\n",
    "def apply_blur(gray_face_list):\n",
    "    blur_face_list = []\n",
    "    for face in gray_face_list:\n",
    "        if face.shape[0] == 0 or face.shape[1] == 0:\n",
    "            gray_face_list.remove(face)\n",
    "            continue\n",
    "        blur = cv2.GaussianBlur(face, (7, 7), 0)\n",
    "        blur_face_list.append(blur)\n",
    "    return blur_face_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a clean training directory for the specified character '''\n",
    "\n",
    "def make_training_directory(character, path):\n",
    "    training_path = path + '/' + character + '/training'\n",
    "\n",
    "    # Create a fresh training directory for each character\n",
    "    try:\n",
    "        _ = os.mkdir(training_path)\n",
    "    except FileExistsError:\n",
    "        _ = shutil.rmtree(training_path)\n",
    "        _ = os.mkdir(training_path)\n",
    "    \n",
    "    return training_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Apply Augmentor to images stored in project directory to generate num_samples more images'''\n",
    "\n",
    "def augment_images(path_name, num_samples=100, random_state=None):\n",
    "    random.seed(random_state)\n",
    "    path_to_images = glob(path_name) \n",
    "    for path in path_to_images:\n",
    "\n",
    "        # Delete output directories if they exist already\n",
    "        output_path = path + '/output'\n",
    "        try:\n",
    "            shutil.rmtree(output_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "        # Implement Augmentor pipeline to each character image directory w. random prob of each step occurring\n",
    "        p = Augmentor.Pipeline(path)\n",
    "        p.rotate(random.uniform(0.25, 0.75), max_left_rotation=10, max_right_rotation=10)\n",
    "        p.shear(random.uniform(0.25, 0.75), max_shear_left=1, max_shear_right=0.01)\n",
    "        p.shear(random.uniform(0.25, 0.75), max_shear_left=0.01, max_shear_right=1)\n",
    "        p.flip_left_right(random.uniform(0.25, 0.75))\n",
    "        \n",
    "        # Create specified number of random samples\n",
    "        _ = p.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Perform a train-test split on the given ith-fold with image augmentation on training images'''\n",
    "\n",
    "def train_augment_test_split(character_dict, project_path, data_dict, face_dict, i,\n",
    "                             num_samples, folds, random_state=None):\n",
    "    \n",
    "    test_img_list = []\n",
    "    test_label_list = np.array([])\n",
    "    \n",
    "    # Train-test split on each individual character\n",
    "    for (label, img_list), (_, face_list) in zip(data_dict.items(), face_dict.items()):\n",
    "        list_len = len(img_list)\n",
    "        test_sample_len = int(1/folds * list_len)\n",
    "\n",
    "        # Test Split on face images\n",
    "        test_split = face_list[i*test_sample_len : (i+1)*test_sample_len]\n",
    "        test_label_list = np.append(test_label_list, np.full(len(test_split), label))\n",
    "        test_img_list = test_img_list + test_split\n",
    "\n",
    "        # Train Split on original images\n",
    "        train_split = img_list[:i*test_sample_len] + img_list[(i+1)*test_sample_len:]\n",
    "\n",
    "        # Save training images to training directory\n",
    "        character = character_dict[label]\n",
    "        training_path = make_training_directory(character, project_path)\n",
    "        for img_num, img in enumerate(train_split):\n",
    "            img_path = '{}/{}.png'.format(training_path, img_num)\n",
    "            _ = cv2.imwrite(img_path, img)\n",
    "\n",
    "        # Augment the face images from the training directory and save augmentation to output directory\n",
    "        _ = augment_images(training_path, num_samples, random_state)\n",
    "    \n",
    "    return test_img_list, test_label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The model provided for face embedding will be the FaceNet model generated by Hiroi Taniai.\n",
    "The output is a 128-D embedding vector of the face provided\n",
    "'''\n",
    "def embed_face(face, model, required_size=(160,160)):\n",
    "    \n",
    "    # Turn face into RGB acceptable format for FaceNet model\n",
    "    if face.shape[-1] != 3:\n",
    "        img = cv2.cvtColor(face, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        img = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize the face image to the required 160x160 FaceNet image size    \n",
    "    img = cv2.resize(img, required_size)\n",
    "    img = img.astype('float32')\n",
    "    \n",
    "    # Standardize the pixel values\n",
    "    mean, std = img.mean(), img.std()\n",
    "    img = (img - mean) / std\n",
    "    \n",
    "    # Turn the processed face into a sample that can be used for embedding\n",
    "    sample = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Embed the face to a 128 element vector using the FaceNet model\n",
    "    embedding = model.predict(sample)\n",
    "    \n",
    "    # Return the embedded vector\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Edit the generate_confusion_matrix function to work with image embeddings from FaceNet \n",
    "    (changes provided in else-statement)\n",
    "'''\n",
    "\n",
    "def generate_confusion_matrix(character_dict, test_img_list, test_label_list, \n",
    "                              face_recognizer, test_X=None, test_y=None):\n",
    "    \n",
    "    # Generate initial confusion matrix DataFrame\n",
    "    character_list = []\n",
    "    for _, character in character_dict.items():\n",
    "        character_list.append(character)\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame(index=character_list, columns=character_list)\n",
    "    confusion_matrix.loc[:, :] = 0\n",
    "    \n",
    "    pred_img_df = pd.DataFrame(index=character_list, columns=character_list).astype('object')\n",
    "    for i in range(pred_img_df.shape[0]):\n",
    "        for j in range(pred_img_df.shape[1]):\n",
    "            pred_img_df.iloc[i, j] = []\n",
    "    \n",
    "    # Make predictions on test images\n",
    "    if (test_X is None) and (test_y is None):\n",
    "        for test_img, true_label in zip(test_img_list, test_label_list):\n",
    "\n",
    "            # What to do if the test images are in color \n",
    "            if (test_img.shape[-1] == 3):\n",
    "                # Make prediction on test image and add to pred_face_list\n",
    "                pred_label_list, pred_rect_list = predict(test_img, face_recognizer)\n",
    "\n",
    "                if pred_rect_list is not None:\n",
    "                    pred_img = draw_character_labels(test_img, character_dict, \n",
    "                                                     pred_label_list, pred_rect_list)\n",
    "\n",
    "                # Populate confusion matrix based on prediction\n",
    "                actual_character = character_dict[true_label]\n",
    "                pred_character = character_dict[pred_label_list[0]]\n",
    "\n",
    "                confusion_matrix.loc[pred_character, actual_character] += 1\n",
    "                pred_img_df.loc[pred_character, actual_character].append(pred_img)\n",
    "\n",
    "            # What to do if the test image is in grayscale\n",
    "            else:\n",
    "                # Predict on the grayscale face provided\n",
    "                pred_label = face_recognizer.predict(test_img)\n",
    "\n",
    "                # Populate confusion matrix based on prediction\n",
    "                actual_character = character_dict[true_label]\n",
    "                pred_character = character_dict[pred_label[0]]\n",
    "\n",
    "                confusion_matrix.loc[pred_character, actual_character] += 1\n",
    "                pred_img_df.loc[pred_character, actual_character].append(test_img)\n",
    "    \n",
    "    # Make prediction on image vectors\n",
    "    else:\n",
    "        \n",
    "        # Predict on the testing image set\n",
    "        pred_y = face_recognizer.predict(test_X)\n",
    "        \n",
    "        for ((idx, pred_label), true_label) in zip(enumerate(pred_y), test_y):\n",
    "            \n",
    "            # Add point to confusion matrix\n",
    "            pred_character = character_dict[pred_label]\n",
    "            actual_character = character_dict[true_label]\n",
    "            confusion_matrix.loc[pred_character, actual_character] += 1\n",
    "            \n",
    "            # Add test image to DataFrame\n",
    "            img = test_img_list[idx].copy()\n",
    "            pred_img_df.loc[pred_character, actual_character].append(img)\n",
    "    \n",
    "    return confusion_matrix, pred_img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Edit the CV function to work on image embeddings (face_recognizer takes on the form of FaceNet model)'''\n",
    "\n",
    "def cross_validate(original_list, label_list, face_recognizer, character_dict, \n",
    "                    project_path, preprocess_steps=[normalize_face_image_size],\n",
    "                       num_samples=100, folds=5, random_state=None):\n",
    "    \n",
    "    pred_img_df_list = []\n",
    "    confusion_matrix_list = []\n",
    "    f1_average_list = []\n",
    "    t_list = []\n",
    "    \n",
    "    # Shuffle the images\n",
    "    cv_img_list, cv_label_list = shuffle_original(original_list, label_list, random_state)\n",
    "    \n",
    "    # Prepare an image dictionary with images where the LBPH could detect a face\n",
    "    data_dict, original_face_dict = prepare_image_dictionaries(cv_img_list, \n",
    "                                                               cv_label_list, \n",
    "                                                               [])\n",
    "        \n",
    "    for i in range(folds):\n",
    "            \n",
    "        # Augment the training data and create testing image data\n",
    "        test_face_list, test_label_list = train_augment_test_split(character_dict,\n",
    "                                                                  project_path,\n",
    "                                                                  data_dict, \n",
    "                                                                  original_face_dict, i,\n",
    "                                                                  num_samples, \n",
    "                                                                  folds, random_state)\n",
    "            \n",
    "        # Retrieve augmented image data for training the classifier\n",
    "        training_img_path = project_path + '/*/training/output/*.png' \n",
    "        _, train_img_list, train_label_list = prepare_data(training_img_path, -4)\n",
    "        \n",
    "        # Gather faces from augmented training data\n",
    "        _, augment_face_dict = prepare_image_dictionaries(train_img_list, \n",
    "                                                       train_label_list, \n",
    "                                                       preprocess_steps)\n",
    "        \n",
    "        # Convert the training faces to 128-element embedded FaceNet vectors\n",
    "        embed_augment_face_dict = defaultdict(list)\n",
    "        for key, face_list in augment_face_dict.items():\n",
    "            embedding_face_list = []\n",
    "            for face in face_list:\n",
    "                if face is None or face.shape[0] == 0 or face.shape[1] == 0:\n",
    "                    continue\n",
    "                embedding = embed_face(face, face_recognizer)\n",
    "                embedding_face_list.append(embedding)\n",
    "            embed_augment_face_dict[key] = embedding_face_list\n",
    "        \n",
    "        \n",
    "        # Convert the testing faces to 128-element embedded FaceNet vectors\n",
    "        test_face_dict = defaultdict(list)\n",
    "        for img, label in zip(test_face_list, test_label_list):\n",
    "            test_face_dict[label].append(img)\n",
    "        embed_test_face_dict = defaultdict(list)\n",
    "        test_face_list = []\n",
    "        test_face_label_list = np.array([])\n",
    "        for label, face_list in test_face_dict.items():\n",
    "            embedding_face_list = []\n",
    "            for face in face_list:\n",
    "                if face is None or face.shape[0] == 0 or face.shape[1] == 0:\n",
    "                    continue\n",
    "                embedding = embed_face(face, face_recognizer)\n",
    "                embedding_face_list.append(embedding)\n",
    "                test_face_list.append(face)\n",
    "                test_face_label_list = np.append(test_face_label_list, label)\n",
    "            embed_test_face_dict[label] = embedding_face_list\n",
    "\n",
    "        \n",
    "        # Generate the final training embedding list and labels from embed_augment_face_dict\n",
    "        train_X = []\n",
    "        train_y = np.array([])\n",
    "        for (label, embed_list) in embed_augment_face_dict.items():\n",
    "            for embed in embed_list:\n",
    "                train_X.append(embed)\n",
    "            train_y = np.append(train_y, np.full(len(embed_list), label))\n",
    "        train_X = np.asarray(train_X)\n",
    "        \n",
    "        # Generate the final testing embedding list and labels from the embed_test_face_dict\n",
    "        test_X = []\n",
    "        test_y = np.array([])\n",
    "        for (label, embed_list) in embed_test_face_dict.items():\n",
    "            for embed in embed_list:\n",
    "                test_X.append(embed)\n",
    "            test_y = np.append(test_y, np.full(len(embed_list), label))\n",
    "        test_X = np.asarray(test_X)\n",
    "        \n",
    "        \n",
    "        # Cast training label list as integer list and train the classifier\n",
    "        train_y = train_y.astype(int)\n",
    "        test_y = test_y.astype(int)\n",
    "        \n",
    "        # Normalize input vectors\n",
    "        norm = Normalizer(norm='l2')\n",
    "        train_X = norm.transform(train_X)\n",
    "        test_X = norm.transform(test_X)\n",
    "        \n",
    "        # Train a linear SVM for facial recognition\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(train_X, train_y)\n",
    "        \n",
    "        # Generate the confusion matrix by predicting test images\n",
    "        confusion_matrix, pred_img_df = generate_confusion_matrix(character_dict, \n",
    "                                                                  test_face_list, test_face_label_list, \n",
    "                                                                  model, test_X, test_y)\n",
    "        confusion_matrix_list.append(confusion_matrix)\n",
    "        pred_img_df_list.append(pred_img_df)\n",
    "        \n",
    "        # Gather f1 score information from data gathered in testing phase\n",
    "        f1_score_list = generate_f1_scores(confusion_matrix, character_dict)\n",
    "        character_average_f1 = np.mean(f1_score_list)\n",
    "        f1_average_list.append(character_average_f1)\n",
    "        t_list.append(test_face_list)\n",
    "        \n",
    "    # Average the f1 scores together\n",
    "    cv_average_f1 = np.mean(f1_average_list)\n",
    "    \n",
    "    return pred_img_df_list, confusion_matrix_list, f1_average_list, cv_average_f1, t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multi-Task Convolutional Neural Network (MTCNN) Facial Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Detect faces from a photograph using MTCNN'''\n",
    "\n",
    "def detect_faces(img, detector):\n",
    "    \n",
    "    rect_coord_list = []\n",
    "    faces_list = []\n",
    "\n",
    "    # Run facial detection on the image\n",
    "    faces = detector.detect_faces(img)\n",
    "    if len(faces)==0:\n",
    "        return None, None\n",
    "    \n",
    "    # Create face and rectangle coordinate lists from facial detection\n",
    "    for face in faces:\n",
    "        (x, y, w, h) = face['box']\n",
    "        if h == 0 or w == 0:\n",
    "            return None, None\n",
    "        faces_list.append(img[y:y+h, x:x+w])\n",
    "        rect_coord_list.append((x, y, w, h))\n",
    "    \n",
    "    # Return face image and coordinates where the face was found\n",
    "    return faces_list, rect_coord_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Edit the prepare_image_dictionaries function to use MTCNN when detecting faces '''\n",
    "\n",
    "def prepare_image_dictionaries(original_list, label_list, preprocess_steps):\n",
    "    \n",
    "    img_dict = defaultdict(list)\n",
    "    face_dict = defaultdict(list)\n",
    "    super_face_list = []\n",
    "    detector = MTCNN()\n",
    "    \n",
    "    # Organize the original image and face lists by their corresponding labels\n",
    "    for img, label in zip(original_list, label_list):\n",
    "        faces, _ = detect_faces(img, detector)\n",
    "        if faces is not None:\n",
    "            img_dict[label].append(img)\n",
    "            face_dict[label].append(faces[0])\n",
    "    \n",
    "    # Add all the faces from the face dictionary to a gray_face_list super list\n",
    "    for _, face_list in face_dict.items():\n",
    "        for face in face_list:\n",
    "            super_face_list.append(face)\n",
    "    \n",
    "    # Process all the faces in the super list\n",
    "    process_face_list = preprocess_images(super_face_list, preprocess_steps)\n",
    "    \n",
    "    # Store the processed faces to the face dictionary\n",
    "    start = end = 0\n",
    "    for label, img_list in img_dict.items():\n",
    "        start, end = end, end + len(img_list)\n",
    "        face_dict[label] = process_face_list[start:end]\n",
    "  \n",
    "    return img_dict, face_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Augment the images from the training path that you want to train with'''\n",
    "\n",
    "def augment_training_images(character_dict, project_path, img_dict,\n",
    "                             num_samples=100, random_state=None):\n",
    "        \n",
    "    # Gather all the original images for a character each individual character\n",
    "    for (label, img_list) in img_dict.items():\n",
    "\n",
    "        # Train on all the original images\n",
    "        train_split = img_list\n",
    "\n",
    "        # Save training images to training directory\n",
    "        character = character_dict[label]\n",
    "        training_path = make_training_directory(character, project_path)\n",
    "        for img_num, face in enumerate(train_split):\n",
    "            img_path = '{}/{}.png'.format(training_path, img_num)\n",
    "            _ = cv2.imwrite(img_path, face)\n",
    "\n",
    "        # Augment the face images from the training directory and save augmentation to output directory\n",
    "        _ = augment_images(training_path, num_samples, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract features from images using FaceNet'''\n",
    "def embed_training_images(project_path, feature_extractor):\n",
    "     \n",
    "        # Retrieve augmented image data for img embedding\n",
    "        training_img_path = project_path + '/*/training/output/*.png' \n",
    "        _, train_img_list, train_label_list = prepare_data(training_img_path, -4)\n",
    "        \n",
    "        # Gather faces from augmented training data\n",
    "        _, augment_face_dict = prepare_image_dictionaries(train_img_list, \n",
    "                                                       train_label_list, \n",
    "                                                       [])\n",
    "        \n",
    "        # Convert the training faces to 128-element embedded FaceNet vectors\n",
    "        embed_augment_face_dict = defaultdict(list)\n",
    "        for key, face_list in augment_face_dict.items():\n",
    "            embedding_face_list = []\n",
    "            for face in face_list:\n",
    "                if face is None or face.shape[0] == 0 or face.shape[1] == 0:\n",
    "                    continue\n",
    "                embedding = embed_face(face, feature_extractor)\n",
    "                embedding_face_list.append(embedding)\n",
    "            embed_augment_face_dict[key] = embedding_face_list\n",
    "        \n",
    "        return embed_augment_face_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "testing_img_class used to categorize images with their corresponding faces, face coordinates, and embedded vectors\n",
    "'''\n",
    "\n",
    "class testing_img_class(object):\n",
    "    \n",
    "    def __init__(self, img, face_list, rect_coord_list):\n",
    "        self.img = img\n",
    "        self.face_list = face_list\n",
    "        self.rect_coord_list = rect_coord_list\n",
    "        self.embedded_face_list = self.prediction_list = []\n",
    "    \n",
    "    def add_embedding_list(self, embedding_list):\n",
    "        self.embedded_face_list = embedding_list\n",
    "    \n",
    "    def add_predictions(self, prediction_list):\n",
    "        self.prediction_list = prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generates list of test_img_class class types that store img and faces / coordinates of where face was found '''\n",
    "\n",
    "def detect_faces_from_testing_imgs(test_img_list):\n",
    "    \n",
    "    test_img_class_list = []\n",
    "    detector = MTCNN()\n",
    "    \n",
    "    # Organize the original image and face lists by their corresponding labels\n",
    "    for img in test_img_list:\n",
    "        face_list, rect_coord_list = detect_faces(img, detector)\n",
    "        if face_list is not None and rect_coord_list is not None:\n",
    "            temp = testing_img_class(img, face_list, rect_coord_list)\n",
    "            test_img_class_list.append(temp)\n",
    "  \n",
    "    return test_img_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Adds all the embedded vectors to the test img class list'''\n",
    "def embed_testing_images(test_img_class_list, feature_extractor):\n",
    "        \n",
    "    # Convert the testing faces to 128-element embedded FaceNet vectors\n",
    "    for item in test_img_class_list:\n",
    "        embedding_face_list = []\n",
    "        if item.face_list is None:\n",
    "            continue\n",
    "        for face in item.face_list:\n",
    "            embedding = embed_face(face, feature_extractor)\n",
    "            if embedding is None:\n",
    "                continue\n",
    "            embedding_face_list.append(embedding)\n",
    "        item.add_embedding_list(embedding_face_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train SVM on embeddings from augmented image data'''\n",
    "def train_SVM(embed_augment_face_dict):\n",
    "    \n",
    "    # Generate the final training embedding list and labels from embed_augment_face_dict\n",
    "    train_X = []\n",
    "    train_y = np.array([])\n",
    "    for (label, embed_list) in embed_augment_face_dict.items():\n",
    "        for embed in embed_list:\n",
    "            train_X.append(embed)\n",
    "        train_y = np.append(train_y, np.full(len(embed_list), label))\n",
    "    train_X = np.asarray(train_X)\n",
    "    \n",
    "    # Train a linear SVM for facial recognition\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "    model.fit(train_X, train_y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Run predictions using trained SVM on testing embedding set'''\n",
    "\n",
    "def test_SVM(test_img_class_list, character_dict, model):\n",
    "    for item in test_img_class_list:\n",
    "        # Predict on the testing image set\n",
    "        test_X = item.embedded_face_list\n",
    "        pred_y = model.predict(test_X)\n",
    "        item.add_predictions(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create clean prediction directories for all the characters '''\n",
    "\n",
    "def make_prediction_directories(character_dict, path):\n",
    "    for _, character in character_dict.items():\n",
    "        \n",
    "        prediction_path = path + '/' + character + '/prediction'\n",
    "        # Create a fresh prediction directory for each character\n",
    "        try:\n",
    "            _ = os.makedirs(prediction_path)\n",
    "        except FileExistsError:\n",
    "            _ = shutil.rmtree(prediction_path)\n",
    "            _ = os.makedirs(prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save all the predicted faces to the predicted character's project directory'''\n",
    "\n",
    "def save_faces(test_img_class_list, path, character_dict):\n",
    "    for item in test_img_class_list:\n",
    "        for (face, label) in zip(item.face_list, item.prediction_list):\n",
    "            \n",
    "            # Find the name of the character associated with the label\n",
    "            character = character_dict[label]\n",
    "            \n",
    "            # Save the face to the first available image number name\n",
    "            i = 1\n",
    "            prediction_path = path + '/' + character + '/prediction'\n",
    "            while os.path.exists(\"{}/{}.png\".format(prediction_path, i)):\n",
    "                i += 1\n",
    "            img_path = '{}/{}.png'.format(prediction_path, i)\n",
    "            _ = cv2.imwrite(img_path, face)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:face_project]",
   "language": "python",
   "name": "conda-env-face_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
